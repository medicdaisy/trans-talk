<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Finla.ai - Advanced Transcription UI</title>
    <style>
        /* CSS Variables for Theming and Consistency */
        :root {
            --font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            
            /* Finla.ai Inspired Colors */
            --finla-dark-blue: #1E3A8A; 
            --finla-light-blue-accent: #60A5FA;
            --finla-green-accent: #34D399;

            /* Light Theme (Default) */
            --bg-primary-light: #F7F9FC;
            --bg-secondary-light: #FFFFFF;
            --text-primary-light: #222F3E;
            --text-secondary-light: #576574;
            --accent-primary-light: var(--finla-dark-blue);
            --accent-secondary-light: #1C3274;
            --border-color-light: #DDE3EA;
            --shadow-color-light: rgba(30, 58, 138, 0.08);
            --error-color-light: #EF4444;
            --success-color-light: #10B981;
            --output-bg-light: #FDFEFE;


            /* Dark Theme */
            --bg-primary-dark: #161A1D;
            --bg-secondary-dark: #1F2428;
            --text-primary-dark: #E5E7EB;
            --text-secondary-dark: #9CA3AF;
            --accent-primary-dark: var(--finla-light-blue-accent);
            --accent-secondary-dark: #3B82F6;
            --border-color-dark: #374151;
            --shadow-color-dark: rgba(0, 0, 0, 0.2);
            --error-color-dark: #F87171;
            --success-color-dark: #34D399;
            --output-bg-dark: #24292E;


            /* Universal Variables */
            --border-radius: 8px;
            --transition-speed: 0.25s;
            --button-padding: 0.75em 1.4em;
        }

        /* Initialize theme variables */
        body {
            --bg-primary: var(--bg-primary-light);
            --bg-secondary: var(--bg-secondary-light);
            --text-primary: var(--text-primary-light);
            --text-secondary: var(--text-secondary-light);
            --accent-primary: var(--accent-primary-light);
            --accent-secondary: var(--accent-secondary-light);
            --border-color: var(--border-color-light);
            --shadow-color: var(--shadow-color-light);
            --error-color: var(--error-color-light);
            --success-color: var(--success-color-light);
            --output-bg: var(--output-bg-light);
        }

        body.dark-theme {
            --bg-primary: var(--bg-primary-dark);
            --bg-secondary: var(--bg-secondary-dark);
            --text-primary: var(--text-primary-dark);
            --text-secondary: var(--text-secondary-dark);
            --accent-primary: var(--accent-primary-dark);
            --accent-secondary: var(--accent-secondary-dark);
            --border-color: var(--border-color-dark);
            --shadow-color: var(--shadow-color-dark);
            --error-color: var(--error-color-dark);
            --success-color: var(--success-color-dark);
            --output-bg: var(--output-bg-dark);
        }

        /* Global Styles */
        *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
        html { scroll-behavior: smooth; }
        body {
            font-family: var(--font-family); background-color: var(--bg-primary); color: var(--text-primary);
            transition: background-color var(--transition-speed) ease, color var(--transition-speed) ease;
            font-size: 16px; line-height: 1.6;
        }
        .container { width: 100%; max-width: 1200px; margin: 0 auto; padding: 1rem; }

        /* Header */
        .app-header {
            display: flex; justify-content: space-between; align-items: center; padding: 0.75rem 1rem;
            background-color: var(--bg-secondary); box-shadow: 0 2px 8px var(--shadow-color);
            border-bottom: 1px solid var(--border-color); position: sticky; top: 0; z-index: 1000;
            transition: background-color var(--transition-speed) ease, border-color var(--transition-speed) ease;
        }
        .header-actions {
            display: flex;
            align-items: center;
            gap: 1rem; /* Space between theme switch and new icon */
        }
        .voice-agent-link {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 36px; /* Adjust size as needed */
            height: 36px; /* Adjust size as needed */
            border-radius: 50%;
            background-color: var(--accent-primary);
            color: white;
            text-decoration: none;
            transition: background-color 0.25s ease, transform 0.1s ease;
            box-shadow: 0 2px 4px var(--shadow-color-light);
        }
        .voice-agent-link:hover {
            background-color: var(--accent-secondary);
            transform: scale(1.05);
            box-shadow: 0 4px 8px var(--shadow-color-light);
        }
        body.dark-theme .voice-agent-link {
            box-shadow: 0 2px 4px var(--shadow-color-dark);
        }
        body.dark-theme .voice-agent-link:hover {
            box-shadow: 0 4px 8px var(--shadow-color-dark);
        }
        .voice-agent-link svg {
            width: 20px; /* Icon size */
            height: 20px; /* Icon size */
            fill: currentColor;
        }


        .theme-switch-wrapper { display: flex; align-items: center; }
        .theme-switch { display: inline-block; height: 26px; position: relative; width: 50px; margin-left: 0.5rem; }
        .theme-switch input { display:none; }
        .slider {
            background-color: #B0B0B0; bottom: 0; cursor: pointer; left: 0; position: absolute; right: 0; top: 0;
            transition: .4s; border-radius: 26px;
        }
        .slider:before {
            background-color: #fff; bottom: 3px; content: ""; height: 20px; left: 3px; position: absolute;
            transition: .4s; width: 20px; border-radius: 50%; box-shadow: 0 1px 3px rgba(0,0,0,0.2);
        }
        input:checked + .slider { background-color: var(--accent-primary); }
        input:checked + .slider:before { transform: translateX(24px); }
        .main-content { padding-top: 1.5rem; display: flex; flex-direction: column; gap: 1.5rem; }
        .card {
            background-color: var(--bg-secondary); border-radius: var(--border-radius); padding: 1.5rem;
            box-shadow: 0 4px 12px var(--shadow-color); border: 1px solid var(--border-color);
            transition: background-color var(--transition-speed) ease, border-color var(--transition-speed) ease, box-shadow var(--transition-speed) ease;
        }
        .card-title { font-size: 1.1rem; font-weight: 600; margin-bottom: 1rem; color: var(--text-primary); }
        .controls-area .control-group { margin-bottom: 1rem; }
        .controls-area label { display: block; font-size: 0.875rem; color: var(--text-secondary); margin-bottom: 0.4rem; }
        .controls-area select, .status-display-text { 
            width: 100%; padding: 0.65rem 0.75rem; border-radius: var(--border-radius); border: 1px solid var(--border-color);
            background-color: var(--bg-primary); color: var(--text-primary); font-size: 0.9rem;
            transition: border-color var(--transition-speed) ease, background-color var(--transition-speed) ease, color var(--transition-speed) ease;
        }
        .controls-area select:focus { outline: none; border-color: var(--accent-primary); box-shadow: 0 0 0 2px var(--accent-primary-light); }
        body.dark-theme .controls-area select:focus { box-shadow: 0 0 0 2px var(--accent-primary-dark); }
        
        .topic-focus-group { margin-top: 1rem; }
        .topic-focus-group .card-title { margin-bottom: 0.5rem; font-size:1rem; }
        .topic-focus-group .checkbox-group { display: flex; flex-direction: column; gap: 0.5rem; }
        .topic-focus-group .checkbox-item { display: flex; align-items: center; }
        .topic-focus-group .checkbox-item input[type="checkbox"] { margin-right: 0.5rem; width: 16px; height: 16px; accent-color: var(--accent-primary); }
        .topic-focus-group .checkbox-item label { font-size: 0.9rem; color: var(--text-primary); margin-bottom: 0; }


        .status-display-container { opacity: 1; transform: translateY(0); transition: opacity var(--transition-speed) ease, transform var(--transition-speed) ease; }
        .status-display-container.hidden { opacity: 0; transform: translateY(-10px); height: 0; overflow: hidden; }
        .status-display-text {
             min-height: 38px; display: flex; align-items: center; font-style: italic; margin-bottom: 0.5rem;
        }
        .status-display-text.error { color: var(--error-color); font-weight: 500; border-left: 3px solid var(--error-color); padding-left: 0.5rem;}
        .status-display-text.success { color: var(--success-color); font-weight: 500; border-left: 3px solid var(--success-color); padding-left: 0.5rem;}
        #audioWaveformCanvas {
            width: 100%; height: 60px; background-color: var(--output-bg);
            border-radius: calc(var(--border-radius) / 2); display: none; margin-top: 0.5rem; border: 1px solid var(--border-color);
        }
        .button-group { display: flex; flex-direction: column; gap: 0.75rem; }
        .button-row { display: flex; gap: 0.5rem; } 
        .button-row .btn { flex: 1; } 

        .btn {
            padding: var(--button-padding); font-size: 0.95rem; font-weight: 500; border: none;
            border-radius: var(--border-radius); cursor: pointer;
            transition: background-color var(--transition-speed) ease, transform 0.1s ease, box-shadow var(--transition-speed) ease;
            text-align: center; width: 100%;
            box-shadow: 0 2px 4px var(--shadow-color-light);
        }
        body.dark-theme .btn { box-shadow: 0 2px 4px var(--shadow-color-dark); }
        .btn-primary { background-color: var(--accent-primary); color: white; }
        .btn-primary:hover:not(:disabled) { background-color: var(--accent-secondary); box-shadow: 0 4px 8px var(--shadow-color-light); }
        body.dark-theme .btn-primary:hover:not(:disabled) { box-shadow: 0 4px 8px var(--shadow-color-dark); }
        .btn-secondary { background-color: var(--bg-secondary); color: var(--accent-primary); border: 1.5px solid var(--accent-primary); }
        .btn-secondary:hover:not(:disabled) { background-color: var(--accent-primary); color: white; box-shadow: 0 4px 8px var(--shadow-color-light); }
        body.dark-theme .btn-secondary:hover:not(:disabled) { background-color: var(--accent-primary); color: white; box-shadow: 0 4px 8px var(--shadow-color-dark); }
        .btn-warning { background-color: #F59E0B; color: white; } 
        .btn-warning:hover:not(:disabled) { background-color: #D97706; } 

        .btn:active:not(:disabled) { transform: scale(0.98); box-shadow: 0 1px 2px var(--shadow-color-light); }
        body.dark-theme .btn:active:not(:disabled) { box-shadow: 0 1px 2px var(--shadow-color-dark); }
        .btn.disabled, .btn:disabled {
            background-color: var(--text-secondary-light) !important; color: var(--bg-secondary-light) !important;
            cursor: not-allowed !important; opacity: 0.5 !important; border-color: var(--text-secondary-light) !important;
            box-shadow: none !important;
        }
        body.dark-theme .btn.disabled, body.dark-theme .btn:disabled {
             background-color: var(--text-secondary-dark) !important; color: var(--bg-secondary-dark) !important;
             border-color: var(--text-secondary-dark) !important;
        }
        .upload-group { margin-top: 0.75rem; }
        #audioFileUpload { display: none; }
        .btn-upload { background-color: var(--finla-green-accent); color: white; }
        .btn-upload:hover:not(:disabled) { background-color: #25a575; box-shadow: 0 4px 8px var(--shadow-color-light);}
        body.dark-theme .btn-upload:hover:not(:disabled) { box-shadow: 0 4px 8px var(--shadow-color-dark); }
        #fileNameDisplay { font-size: 0.8rem; color: var(--text-secondary); margin-top: 0.4rem; text-align: center; word-break: break-all; }
        .output-column { display: flex; flex-direction: column; gap: 1.5rem; }
        .output-area {
            width: 100%; min-height: 120px; padding: 0.75rem 1rem; border-radius: var(--border-radius);
            border: 1px solid var(--border-color); background-color: var(--output-bg); color: var(--text-primary);
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
            font-size: 0.875rem; line-height: 1.6; resize: vertical;
            transition: border-color var(--transition-speed) ease, background-color var(--transition-speed) ease;
            white-space: pre-wrap; word-wrap: break-word;
        }
        .output-area:focus { outline: none; border-color: var(--accent-primary); box-shadow: 0 0 0 2px var(--accent-primary-light); }
        body.dark-theme .output-area:focus { box-shadow: 0 0 0 2px var(--accent-primary-dark); }
        
        #detectedTopicsList { list-style-type: none; padding-left: 0; }
        #detectedTopicsList li {
            background-color: var(--bg-primary); padding: 0.5rem 0.75rem; border-radius: calc(var(--border-radius) / 2);
            margin-bottom: 0.5rem; border: 1px solid var(--border-color); font-size: 0.9rem;
            color: var(--text-primary);
        }
         #detectedTopicsList li:last-child { margin-bottom: 0; }
         #detectedTopicsList.empty { font-style: italic; color: var(--text-secondary); }
         .placeholder-note { font-size: 0.8rem; color: var(--text-secondary); margin-top: 0.5rem; font-style: italic;}


        @media (min-width: 768px) {
            .container { padding: 1.5rem 2rem; }
            .main-content { flex-direction: row; align-items: flex-start; }
            .controls-column { flex: 0 0 320px; max-width: 320px; display: flex; flex-direction: column; gap: 1.5rem; }
            .controls-column .card { width: 100%; }
            .transcript-column-wrapper { flex: 1; min-width: 0; }
        }
        .app-footer {
            text-align: center; padding: 1.5rem 1rem; color: var(--text-secondary); font-size: 0.875rem;
            border-top: 1px solid var(--border-color); margin-top: 2rem;
        }
        #loadingOverlay {
            position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            background-color: rgba(0, 0, 0, 0.6);
            display: flex; flex-direction: column; justify-content: center; align-items: center;
            z-index: 2000; color: white; font-size: 1.2rem; text-align: center;
            opacity: 0; visibility: hidden; transition: opacity 0.3s ease, visibility 0.3s ease;
        }
        #loadingOverlay.visible { opacity: 1; visibility: visible; }
        .spinner {
            border: 5px solid #f3f3f3; border-top: 5px solid var(--accent-primary-dark);
            border-radius: 50%; width: 50px; height: 50px;
            animation: spin 1s linear infinite; margin-bottom: 1rem;
        }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }

        .btn.animating {
            animation: btn-pulse-border 1.5s infinite, btn-pulse-bg 1.5s infinite;
        }
        @keyframes btn-pulse-border {
            0% { border-color: var(--accent-primary); }
            50% { border-color: var(--accent-secondary); }
            100% { border-color: var(--accent-primary); }
        }
        @keyframes btn-pulse-bg {
            0% { background-color: var(--accent-primary); }
            50% { background-color: var(--accent-secondary); }
            100% { background-color: var(--accent-primary); }
        }
        .btn-primary.animating {}
        .btn-secondary.animating { 
            background-color: var(--accent-primary); 
            color: white; 
            animation: btn-pulse-border 1.5s infinite, btn-pulse-bg 1.5s infinite; 
        }
        .btn-upload.animating {
            animation: btn-pulse-border-upload 1.5s infinite, btn-pulse-bg-upload 1.5s infinite;
        }
        @keyframes btn-pulse-border-upload {
            0% { border-color: var(--finla-green-accent); }
            50% { border-color: #25a575; }
            100% { border-color: var(--finla-green-accent); }
        }
        @keyframes btn-pulse-bg-upload {
            0% { background-color: var(--finla-green-accent); }
            50% { background-color: #25a575; }
            100% { background-color: var(--finla-green-accent); }
        }

    </style>
</head>
<body>
    <div id="loadingOverlay">
        <div class="spinner"></div>
        <p id="loadingMessage">Processing audio...</p>
    </div>

    <header class="app-header">
        <img src="https://finla.ai/img/logo_text.svg" alt="Finla.ai Logo" class="logo-img">
        <div class="header-actions">
            <a href="voice-agent.html" class="voice-agent-link" title="Launch Voice Agent">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                    <path d="M12 14c1.66 0 2.99-1.34 2.99-3L15 5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.3-3c0 3.53-2.64 6.46-6 6.92V21h2c.55 0 1 .45 1 1s-.45 1-1 1H7c-.55 0-1-.45-1-1s.45-1 1-1h2v-3.08c-3.36-.46-6-3.39-6-6.92h2c0 2.89 2.16 5.31 5 5.72V14z"/>
                </svg>
            </a>
            <div class="theme-switch-wrapper">
                <label for="themeToggleCheckbox" style="color: var(--text-secondary); font-size: 0.8rem; margin-right: 0.5rem;">Dark Mode</label>
                <label class="theme-switch" for="themeToggleCheckbox">
                    <input type="checkbox" id="themeToggleCheckbox" />
                    <div class="slider"></div>
                </label>
            </div>
        </div>
    </header>

    <div class="container">
        <main class="main-content">
            <div class="controls-column">
                <section class="card controls-area">
                    <h2 class="card-title">Transcription Controls</h2>
                    <div class="control-group">
                        <label for="sttServiceSelect">STT Service</label>
                        <select id="sttServiceSelect">
                            <option value="openai_whisper">OpenAI Whisper</option>
                            <option value="deepgram_nova">Deepgram (Nova-3)</option>
                        </select>
                    </div>
                    <div class="button-group">
                        <button id="startRecordBtn" class="btn btn-primary">Start Recording</button>
                        <div class="button-row">
                            <button id="pauseRecordBtn" class="btn btn-warning disabled" disabled>Pause</button>
                            <button id="resumeRecordBtn" class="btn btn-primary disabled" disabled>Resume</button>
                        </div>
                        <button id="stopRecordBtn" class="btn btn-secondary disabled" disabled>Stop & Transcribe</button>
                        <div class="upload-group">
                            <button id="uploadAudioBtn" class="btn btn-upload">Upload Audio File</button>
                            <input type="file" id="audioFileUpload" accept="audio/*,video/webm,audio/webm,audio/mp3,audio/mp4,audio/mpeg,audio/mpga,audio/m4a,audio/wav,audio/ogg">
                            <div id="fileNameDisplay">No file selected.</div>
                        </div>
                    </div>
                     <div class="topic-focus-group control-group">
                        <h3 class="card-title">Topic Focus (Keywords)</h3>
                        <div class="checkbox-group">
                            <div class="checkbox-item">
                                <input type="checkbox" id="topicMedical" name="topicFocus" value="medical">
                                <label for="topicMedical">Medical (Singapore)</label>
                            </div>
                            <div class="checkbox-item">
                                <input type="checkbox" id="topicInsurance" name="topicFocus" value="insurance">
                                <label for="topicInsurance">Insurance (Singapore)</label>
                            </div>
                            <div class="checkbox-item">
                                <input type="checkbox" id="topicGeneral" name="topicFocus" value="general" checked>
                                <label for="topicGeneral">General</label>
                            </div>
                        </div>
                         <p class="placeholder-note" style="font-size:0.75rem; margin-top:0.3rem;">For Deepgram, these selected topics are sent as hints. Deepgram's native topic detection is also enabled.</p>
                    </div>
                </section>

                <section class="card status-area">
                    <h2 class="card-title">Status</h2>
                    <div class="status-display-container">
                        <div id="statusDisplayText" class="status-display-text">Idle. Ready to record or upload.</div>
                    </div>
                    <canvas id="audioWaveformCanvas"></canvas>
                </section>
            </div>

            <div class="transcript-column-wrapper output-column">
                <section class="card">
                    <h2 class="card-title">Conversation (Plain Text)</h2>
                    <textarea id="transcriptOutputConvo" class="output-area" placeholder="Full conversation text will appear here..." readonly></textarea>
                </section>
                <section class="card">
                    <h2 class="card-title">Timestamped Segments</h2>
                    <textarea id="transcriptOutputTimestamped" class="output-area" placeholder="[0.00s - 5.32s] Segment text..." readonly></textarea>
                </section>
                <section class="card">
                    <h2 class="card-title">Multispeaker Output</h2>
                    <textarea id="transcriptOutputMultispeaker" class="output-area" placeholder="[Speaker X: 0.00s - 5.32s] Segment text... (Deepgram provides speaker labels)" readonly></textarea>
                    <p class="placeholder-note">Note: OpenAI Whisper output here will be time-segmented only. Deepgram provides speaker labels.</p>
                </section>
                <section class="card">
                    <h2 class="card-title">Detected Topics</h2>
                    <ul id="detectedTopicsList" class="empty">
                        <li>No topics detected yet.</li>
                    </ul>
                     <p class="placeholder-note">Note: Deepgram provides its own topic detection. For OpenAI, topics are based on client-side keyword matching of selected focus areas.</p>
                </section>
            </div>
        </main>
    </div>

    <footer class="app-footer">
        <p>Â© <span id="currentYear"></span> Finla.ai. For local testing and demonstration.</p>
    </footer>

    <script src="https://www.gstatic.com/firebasejs/9.6.0/firebase-app-compat.js"></script>
    <script src="https://www.gstatic.com/firebasejs/9.6.0/firebase-storage-compat.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const OPENAI_API_KEY = "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"; 
            const DEEPGRAM_API_KEY = "Token 2675a4ae08506fe90d9e110e325252eece2ee43c"; 

            const firebaseConfig = {
              apiKey: "AIzaSyCjVuc2VD5YvJE_4PBUJATmKiJzFC1ex8c",
              authDomain: "aitek2023-8f504.firebaseapp.com",
              databaseURL: "https://aitek2023-8f504-default-rtdb.firebaseio.com",
              projectId: "aitek2023-8f504",
              storageBucket: "aitek2023-8f504.appspot.com",
              messagingSenderId: "570516064142",
              appId: "1:570516064142:web:383ef4de00b5f48f5886df",
              measurementId: "G-PFSD6YN1TV"
            };

            firebase.initializeApp(firebaseConfig);
            const storage = firebase.storage();
            const storageRef = storage.ref();

            const MAX_CHUNK_DURATION_SECONDS_OPENAI = 100; 
            const MEDIA_RECORDER_TIMESLICE_MS = 3000; 

            const loadingOverlay = document.getElementById('loadingOverlay');
            const loadingMessage = document.getElementById('loadingMessage');
            const themeToggleCheckbox = document.getElementById('themeToggleCheckbox');
            const statusDisplayText = document.getElementById('statusDisplayText');
            const startRecordBtn = document.getElementById('startRecordBtn');
            const pauseRecordBtn = document.getElementById('pauseRecordBtn');
            const resumeRecordBtn = document.getElementById('resumeRecordBtn');
            const stopRecordBtn = document.getElementById('stopRecordBtn');
            const sttServiceSelect = document.getElementById('sttServiceSelect'); 
            
            const transcriptOutputConvo = document.getElementById('transcriptOutputConvo');
            const transcriptOutputTimestamped = document.getElementById('transcriptOutputTimestamped');
            const transcriptOutputMultispeaker = document.getElementById('transcriptOutputMultispeaker'); 
            const detectedTopicsList = document.getElementById('detectedTopicsList'); 

            const topicMedicalCheckbox = document.getElementById('topicMedical');
            const topicInsuranceCheckbox = document.getElementById('topicInsurance');
            const topicGeneralCheckbox = document.getElementById('topicGeneral');

            const uploadAudioBtn = document.getElementById('uploadAudioBtn');
            const audioFileUpload = document.getElementById('audioFileUpload');
            const fileNameDisplay = document.getElementById('fileNameDisplay');
            
            const waveformCanvas = document.getElementById('audioWaveformCanvas');
            const waveformCtx = waveformCanvas.getContext('2d');
            
            let audioContext; 
            let vadAnalyserNode; 
            let vadProcessorNode; 
            let mediaStreamSourceForWaveform;
            let mediaStreamSourceForVAD; 
            let analyserForWaveform;
            let dataArrayForWaveform;
            let animationFrameId;

            let mediaRecorder;
            let allRecordedBlobs = []; 
            let currentStream = null; 
            let recordingState = 'idle';

            const DB_NAME = 'FinlaAudioDB_VAD_Firebase_Topics_DG'; 
            const STORE_NAME = 'audioFiles';
            let db;

            const VAD_SILENCE_THRESHOLD = 0.01; 
            const VAD_MIN_SILENCE_DURATION_MS = 1000; 
            const VAD_MIN_SPEECH_DURATION_MS = 200;   
            let vadIsSpeaking = false;
            let vadSilenceStartTime = 0;
            let vadSpeechStartTime = 0;
            let activeSpeechSegments = []; 

            const TOPIC_KEYWORDS_LOCAL = { 
                medical: [
                    'doctor', 'clinic', 'hospital', 'polyclinic', 'singhealth', 'nuhs', 'healthhub', 'medisave', 'medishield', 
                    'appointment', 'prescription', 'diagnosis', 'treatment', 'referral', 'medical certificate', 'mc', 
                    'vaccination', 'health screening', 'physiotherapy', 'ward', 'icu', 'a&e', 'emergency', 'specialist',
                    'general practitioner', 'gp', 'pharmacy', 'medication', 'symptom', 'illness', 'disease', 'insurance claim health',
                    'integrated shield plan', 'careshield', 'cancer', 'diabetes', 'flu', 'covid', 'vaccine', 'surgery'
                ],
                insurance: [
                    'insurance', 'policy', 'premium', 'coverage', 'claim', 'underwriting', 'beneficiary', 'sum assured', 
                    'life insurance', 'health insurance', 'general insurance', 'car insurance', 'travel insurance', 'critical illness',
                    'disability income', 'investment-linked policy', 'ilp', 'cpfis', 'agent', 'broker', 'insurer', 'prudential',
                    'aia', 'great eastern', 'income', 'manulife', 'aviva', 'singlife', 'policyholder', 'rider', 'term life', 'whole life',
                    'endowment', 'annuity', 'dependants protection scheme', 'dps', 'fire insurance', 'accident', 'disability'
                ]
            };

            function initDB() { 
                 return new Promise((resolve, reject) => {
                    const request = indexedDB.open(DB_NAME, 1);
                    request.onerror = event => {
                        console.error("IndexedDB error:", event.target.errorCode, event.target.error);
                        reject("IndexedDB error: " + event.target.errorCode);
                    };
                    request.onsuccess = event => {
                        db = event.target.result;
                        resolve(db);
                    };
                    request.onupgradeneeded = event => {
                        const store = event.target.result.createObjectStore(STORE_NAME, { keyPath: 'id', autoIncrement: true });
                        store.createIndex('name', 'name', { unique: false });
                        store.createIndex('timestamp', 'timestamp', { unique: false });
                    };
                });
            }
            async function saveAudioToDB(blob, name) { 
                 if (!db) {
                    try {
                        await initDB(); 
                        if(!db) { 
                             console.error("DB not initialized for saving, and re-init failed.");
                             updateStatus("Error: DB not ready for saving audio.", "error");
                             return;
                        }
                    } catch (e) {
                        console.error("DB initialization failed during save attempt:", e);
                        updateStatus("Error: DB not ready for saving audio.", "error");
                        return;
                    }
                }
                return new Promise((resolve, reject) => {
                    const transaction = db.transaction([STORE_NAME], 'readwrite');
                    const store = transaction.objectStore(STORE_NAME);
                    const audioRecord = {
                        name: name,
                        blob: blob,
                        timestamp: new Date().toISOString()
                    };
                    const request = store.add(audioRecord);
                    request.onsuccess = () => {
                        console.log(`Audio "${name}" saved to IndexedDB.`);
                        resolve(request.result); 
                    };
                    request.onerror = event => {
                        console.error("Error saving audio to IndexedDB:", event.target.error);
                        updateStatus(`Error saving audio to local DB: ${event.target.error.message}`, "error");
                        reject(event.target.error);
                    };
                });
            }
            async function uploadAudioToFirebase(blob, firebasePathWithName) {
                const audioRef = storageRef.child(firebasePathWithName);
                updateStatus(`Saving "${firebasePathWithName.split('/').pop()}" to cloud storage...`, "info");
                try {
                    const snapshot = await audioRef.put(blob);
                    const downloadURL = await snapshot.ref.getDownloadURL();
                    console.log(`Uploaded "${firebasePathWithName}" to Firebase Storage: ${downloadURL}`);
                    updateStatus(`Audio saved to cloud: ${firebasePathWithName.split('/').pop()}`, "success");
                    return downloadURL;
                } catch (error) {
                    console.error("Error uploading to Firebase Storage:", error);
                    updateStatus(`Error saving to cloud: ${error.message.substring(0,100)}`, "error");
                    throw error; 
                }
             }
            
            initDB().then(() => console.log("IndexedDB initialized successfully."))
                    .catch(err => console.error("Failed to initialize IndexedDB:", err));

            document.getElementById('currentYear').textContent = new Date().getFullYear();

            function showLoading(context = "Processing") { 
                if (context === "recording_start") {
                    return; 
                }
                let message;
                switch (context) {
                    case "mic_request": message = "Requesting microphone..."; break;
                    case "finalizing_recording": message = "Finalizing audio..."; break;
                    case "decoding": message = "Decoding audio..."; break;
                    case "uploading_prepare": message = "Preparing upload..."; break;
                    case "uploading_to_firebase": message = "Saving to cloud..."; break;
                    case "transcribing_chunk": message = "Transcribing..."; break; 
                    case "finalizing_transcription": message = "Finalizing transcript..."; break;
                    default: message = "Processing..."; break;
                }
                loadingMessage.textContent = message;
                loadingOverlay.classList.add('visible');
            }
            function hideLoading() { 
                loadingOverlay.classList.remove('visible');
            }

            function applyTheme(theme) { 
                if (theme === 'dark') {
                    document.body.classList.add('dark-theme');
                    themeToggleCheckbox.checked = true;
                } else {
                    document.body.classList.remove('dark-theme');
                    themeToggleCheckbox.checked = false;
                }
             }
            const savedTheme = localStorage.getItem('theme');
            if (savedTheme) applyTheme(savedTheme);
            else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) applyTheme('dark');
            else applyTheme('light'); 
            themeToggleCheckbox.addEventListener('change', function() {
                const theme = this.checked ? 'dark' : 'light';
                applyTheme(theme);
                localStorage.setItem('theme', theme);
            });

            function updateUIForRecordingState() { 
                startRecordBtn.disabled = recordingState !== 'idle';
                pauseRecordBtn.disabled = recordingState !== 'recording';
                resumeRecordBtn.disabled = recordingState !== 'paused';
                stopRecordBtn.disabled = recordingState === 'idle' || recordingState === 'requesting';
                
                uploadAudioBtn.disabled = recordingState !== 'idle';
                sttServiceSelect.disabled = recordingState !== 'idle'; 
                topicMedicalCheckbox.disabled = recordingState !== 'idle';
                topicInsuranceCheckbox.disabled = recordingState !== 'idle';
                topicGeneralCheckbox.disabled = recordingState !== 'idle';


                [startRecordBtn, pauseRecordBtn, resumeRecordBtn, stopRecordBtn, uploadAudioBtn].forEach(btn => {
                    btn.classList.toggle('disabled', btn.disabled);
                });
             }

            function updateStatus(message, type = "info") { 
                statusDisplayText.textContent = message;
                statusDisplayText.className = 'status-display-text'; 
                if (type === "error") statusDisplayText.classList.add('error');
                if (type === "success") statusDisplayText.classList.add('success');
                
                if (type === "error") console.error(`UI Status (Error): ${message}`);
                else console.log(`UI Status: ${message} (Type: ${type})`);
            }

            function clearOutputFields() { 
                transcriptOutputConvo.value = "";
                transcriptOutputTimestamped.value = "";
                transcriptOutputMultispeaker.value = "";
                detectedTopicsList.innerHTML = '<li>No topics detected yet.</li>';
                detectedTopicsList.classList.add('empty');
             }
            
            function resetToIdle(message = "Idle. Ready to record or upload.", type = "info") { 
                recordingState = 'idle';
                updateUIForRecordingState();
                updateStatus(message, type);

                fileNameDisplay.textContent = "No file selected.";
                if(audioFileUpload.value) audioFileUpload.value = ''; 
                allRecordedBlobs = []; 
                activeSpeechSegments = []; 
                vadIsSpeaking = false; 
                vadSilenceStartTime = 0;
                vadSpeechStartTime = 0;

                stopWaveformVisualization(); 
                stopVAD(); 
                waveformCanvas.style.display = 'none';

                if (currentStream) {
                    currentStream.getTracks().forEach(track => track.stop());
                    currentStream = null;
                }
                mediaRecorder = null;
                hideLoading();

                startRecordBtn.textContent = "Start Recording";
                startRecordBtn.classList.remove('animating');
                pauseRecordBtn.textContent = "Pause";
                resumeRecordBtn.textContent = "Resume";
                stopRecordBtn.textContent = "Stop & Transcribe";
                stopRecordBtn.classList.remove('animating');
                uploadAudioBtn.textContent = "Upload Audio File";
                uploadAudioBtn.classList.remove('animating');
             }

            function startWaveformVisualization(stream) { 
                if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') audioContext.resume();
                
                mediaStreamSourceForWaveform = audioContext.createMediaStreamSource(stream);
                analyserForWaveform = audioContext.createAnalyser();
                analyserForWaveform.fftSize = 2048; 
                const bufferLength = analyserForWaveform.frequencyBinCount;
                dataArrayForWaveform = new Uint8Array(bufferLength);
                
                mediaStreamSourceForWaveform.connect(analyserForWaveform);
                waveformCanvas.style.display = 'block';
                drawWaveform();
            }
            function drawWaveform() { 
                if (!analyserForWaveform || !currentStream || !currentStream.active || recordingState === 'idle') { 
                    stopWaveformVisualization(); 
                    return;
                }
                animationFrameId = requestAnimationFrame(drawWaveform); 
                analyserForWaveform.getByteTimeDomainData(dataArrayForWaveform); 

                waveformCtx.fillStyle = getComputedStyle(document.documentElement).getPropertyValue('--output-bg').trim();
                waveformCtx.fillRect(0, 0, waveformCanvas.width, waveformCanvas.height);
                waveformCtx.lineWidth = 2;
                waveformCtx.strokeStyle = getComputedStyle(document.documentElement).getPropertyValue('--accent-primary').trim();
                waveformCtx.beginPath();
                const sliceWidth = waveformCanvas.width * 1.0 / dataArrayForWaveform.length;
                let x = 0;
                for (let i = 0; i < dataArrayForWaveform.length; i++) { 
                    const v = dataArrayForWaveform[i] / 128.0; 
                    const y = v * waveformCanvas.height / 2;
                    if (i === 0) waveformCtx.moveTo(x, y);
                    else waveformCtx.lineTo(x, y);
                    x += sliceWidth;
                }
                waveformCtx.lineTo(waveformCanvas.width, waveformCanvas.height / 2);
                waveformCtx.stroke();
            }
            function stopWaveformVisualization() {
                if (animationFrameId) cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
                if (waveformCtx) waveformCtx.clearRect(0, 0, waveformCanvas.width, waveformCanvas.height);
                if (mediaStreamSourceForWaveform) { try { mediaStreamSourceForWaveform.disconnect(); } catch(e) {} }
                if (analyserForWaveform) { try { analyserForWaveform.disconnect(); } catch(e) {} }
             }

            function startVAD(stream) { 
                if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') audioContext.resume();

                mediaStreamSourceForVAD = audioContext.createMediaStreamSource(stream);
                vadAnalyserNode = audioContext.createAnalyser();
                vadAnalyserNode.fftSize = 512; 
                vadAnalyserNode.smoothingTimeConstant = 0.5; 

                if (typeof audioContext.createScriptProcessor !== 'function') {
                    console.warn("audioContext.createScriptProcessor is not available. VAD will not run.");
                    updateStatus("VAD (advanced) not available in this browser.", "info");
                    return; 
                }

                const bufferSize = vadAnalyserNode.fftSize;
                vadProcessorNode = audioContext.createScriptProcessor(bufferSize, 1, 1);
                const vadDataArray = new Uint8Array(vadAnalyserNode.frequencyBinCount);

                vadProcessorNode.onaudioprocess = function(audioProcessingEvent) {
                    if (recordingState !== 'recording' || !vadProcessorNode) return; 

                    vadAnalyserNode.getByteFrequencyData(vadDataArray); 
                    
                    let sum = 0;
                    for (let i = 0; i < vadDataArray.length; i++) {
                        sum += vadDataArray[i];
                    }
                    const averageEnergy = sum / vadDataArray.length / 255; 

                    const currentTime = audioContext.currentTime; 

                    if (averageEnergy > VAD_SILENCE_THRESHOLD) { 
                        if (!vadIsSpeaking) { 
                            vadIsSpeaking = true;
                            vadSpeechStartTime = currentTime;
                        }
                        vadSilenceStartTime = 0; 
                    } else { 
                        if (vadIsSpeaking) { 
                            if (vadSpeechStartTime > 0 && (currentTime - vadSpeechStartTime) * 1000 >= VAD_MIN_SPEECH_DURATION_MS) {
                                activeSpeechSegments.push({ start: vadSpeechStartTime, end: currentTime });
                            }
                            vadIsSpeaking = false;
                            vadSpeechStartTime = 0; 
                            vadSilenceStartTime = currentTime;
                        }
                    }
                };

                mediaStreamSourceForVAD.connect(vadAnalyserNode);
                vadAnalyserNode.connect(vadProcessorNode);
                vadProcessorNode.connect(audioContext.destination); 
                console.log("VAD System Initialized & Started");
            }
            function stopVAD() { 
                if (vadIsSpeaking && vadSpeechStartTime > 0 && audioContext && audioContext.currentTime) { 
                     const currentTime = audioContext.currentTime;
                     if(currentTime > vadSpeechStartTime && (currentTime - vadSpeechStartTime) * 1000 >= VAD_MIN_SPEECH_DURATION_MS) {
                        activeSpeechSegments.push({ start: vadSpeechStartTime, end: currentTime });
                        console.log(`VAD: Final speech segment [${vadSpeechStartTime.toFixed(2)}s - ${currentTime.toFixed(2)}s] on stop.`);
                     }
                }
                vadIsSpeaking = false;
                vadSpeechStartTime = 0;
                vadSilenceStartTime = 0;

                if (mediaStreamSourceForVAD) { try { mediaStreamSourceForVAD.disconnect(); } catch(e){} }
                if (vadAnalyserNode) { try { vadAnalyserNode.disconnect(); } catch(e){} }
                if (vadProcessorNode) { 
                    try { vadProcessorNode.disconnect(); } catch(e){} 
                    vadProcessorNode.onaudioprocess = null; 
                } 
                console.log("VAD System Stopped. All detected speech segments:", activeSpeechSegments);
             }
            
            startRecordBtn.addEventListener('click', async () => { 
                const selectedService = sttServiceSelect.value;
                if (selectedService === 'openai_whisper' && (!OPENAI_API_KEY || OPENAI_API_KEY.includes("YOUR_OPENAI_API_KEY_HERE"))) {
                    updateStatus("OpenAI API Key not configured. Please edit the script.", "error"); return;
                }
                if (selectedService === 'deepgram_nova' && (!DEEPGRAM_API_KEY || DEEPGRAM_API_KEY === "Token YOUR_DEEPGRAM_API_KEY_HERE" || DEEPGRAM_API_KEY.length < 20)) { 
                     updateStatus("Deepgram API Key not configured. Please edit the script.", "error"); return;
                }

                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) { 
                    updateStatus("Microphone access not supported by your browser.", "error"); return; 
                }
                
                clearOutputFields(); 
                activeSpeechSegments = []; 
                updateStatus("Requesting microphone access...", "info"); 
                showLoading("mic_request");
                recordingState = 'requesting'; 
                updateUIForRecordingState();
                startRecordBtn.textContent = "Requesting mic...";

                try {
                    currentStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                    
                    hideLoading(); 
                    recordingState = 'recording'; 
                    updateStatus("Recording...", "info"); 
                    showLoading("recording_start"); 
                    updateUIForRecordingState();
                    startRecordBtn.textContent = "Recording..."; 
                    startRecordBtn.classList.add('animating'); 

                    startWaveformVisualization(currentStream);
                    startVAD(currentStream); 

                    allRecordedBlobs = []; 
                    let options = { mimeType: 'audio/webm;codecs=opus' }; 
                    if (!MediaRecorder.isTypeSupported(options.mimeType)) { 
                        options.mimeType = 'audio/webm';
                        if (!MediaRecorder.isTypeSupported(options.mimeType)) options = {}; 
                    }

                    mediaRecorder = new MediaRecorder(currentStream, options);
                    mediaRecorder.ondataavailable = event => {
                        if (event.data.size > 0) allRecordedBlobs.push(event.data);
                    };

                    mediaRecorder.onstop = async () => {
                        stopVAD(); 
                        startRecordBtn.classList.remove('animating');
                        stopRecordBtn.classList.remove('animating');

                        if (recordingState !== 'stopped_for_processing') { 
                            console.warn("MediaRecorder.onstop called without 'stopped_for_processing' state. Assuming stop for processing.");
                            recordingState = 'stopped_for_processing'; 
                            updateStatus("Recording stopped. Processing audio...", "info");
                            showLoading("finalizing_recording"); 
                        }
                        
                        stopWaveformVisualization(); 
                        waveformCanvas.style.display = 'none';

                        if (allRecordedBlobs.length > 0) {
                            const completeOriginalBlob = new Blob(allRecordedBlobs, { type: allRecordedBlobs[0].type || 'audio/webm' });
                            allRecordedBlobs = []; 

                            const timestamp = new Date().toISOString();
                            const recordingNameBase = `recording-${timestamp}`;
                            const localRecordingName = `${recordingNameBase}.webm`;
                            const firebaseRecordingPath = `recorded_audio/${localRecordingName}`;

                            await saveAudioToDB(completeOriginalBlob, localRecordingName); 
                            try {
                                showLoading("uploading_to_firebase"); 
                                await uploadAudioToFirebase(completeOriginalBlob, firebaseRecordingPath);
                            } catch (fbError) {
                                console.warn("Firebase upload failed but continuing with transcription:", fbError);
                                updateStatus("Cloud save failed, proceeding locally.", "info");
                            } finally {
                                hideLoading(); 
                            }
                            
                            if (activeSpeechSegments.length === 0 && completeOriginalBlob.size > 0) {
                                updateStatus("No speech detected by VAD, transcribing full audio.", "info");
                            }
                            await processAndTranscribeAudio(completeOriginalBlob, localRecordingName);
                        } else { 
                            resetToIdle("No audio data recorded.", "info"); 
                        }
                        
                        if (currentStream) { 
                           currentStream.getTracks().forEach(track => track.stop());
                           currentStream = null;
                        }
                    };
                    mediaRecorder.onerror = (event) => { 
                        console.error("MediaRecorder error:", event.error);
                        resetToIdle(`Recorder error: ${event.error.name}. Check console.`, "error"); 
                        stopVAD(); 
                        startRecordBtn.classList.remove('animating'); 
                        hideLoading();
                    };
                    
                    mediaRecorder.start(MEDIA_RECORDER_TIMESLICE_MS); 
                } catch (err) { 
                    console.error("Microphone access error:", err);
                    let msg = `Mic access error: ${err.name} - ${err.message}.`;
                    if (err.name === "NotAllowedError") msg = "Mic permission denied. Please allow access.";
                    if (err.name === "NotFoundError") msg = "No microphone found.";
                    resetToIdle(msg, "error"); 
                    stopVAD(); 
                    startRecordBtn.classList.remove('animating'); 
                    hideLoading(); 
                }
            });
            pauseRecordBtn.addEventListener('click', () => { 
                if (mediaRecorder && mediaRecorder.state === "recording") { 
                    mediaRecorder.pause(); 
                    recordingState = 'paused'; 
                    updateStatus("Recording paused.", "info"); 
                    updateUIForRecordingState(); 
                    startRecordBtn.textContent = "Paused"; 
                    startRecordBtn.classList.remove('animating'); 
                    if (vadProcessorNode && audioContext) { 
                         try { vadProcessorNode.disconnect(); } catch(e){ console.warn("Error disconnecting VAD processor on pause:", e); }
                    }
                    console.log("VAD processing paused");
                }
            });
            resumeRecordBtn.addEventListener('click', () => { 
                if (mediaRecorder && mediaRecorder.state === "paused") { 
                    mediaRecorder.resume(); 
                    recordingState = 'recording'; 
                    updateStatus("Recording resumed...", "info"); 
                    updateUIForRecordingState(); 
                    startRecordBtn.textContent = "Recording..."; 
                    startRecordBtn.classList.add('animating'); 
                    if (vadProcessorNode && vadAnalyserNode && audioContext && audioContext.destination) { 
                        try{
                            vadAnalyserNode.connect(vadProcessorNode); 
                            vadProcessorNode.connect(audioContext.destination); 
                            console.log("VAD processing resumed");
                        } catch(e) {
                            console.warn("Error reconnecting VAD on resume:", e);
                        }
                    }
                }
             });
            stopRecordBtn.addEventListener('click', () => { 
                if (mediaRecorder && (mediaRecorder.state === "recording" || mediaRecorder.state === "paused")) {
                    recordingState = 'stopped_for_processing'; 
                    updateUIForRecordingState(); 
                    startRecordBtn.classList.remove('animating'); 
                    stopRecordBtn.textContent = "Finalizing..."; 
                    showLoading("finalizing_recording"); 
                    mediaRecorder.stop(); 
                } else { 
                    resetToIdle("Not actively recording or already stopped.", "info"); 
                }
            });

            uploadAudioBtn.addEventListener('click', () => { 
                const selectedService = sttServiceSelect.value;
                if (selectedService === 'openai_whisper' && (!OPENAI_API_KEY || OPENAI_API_KEY.includes("YOUR_OPENAI_API_KEY_HERE"))) {
                    updateStatus("OpenAI API Key not configured. Please edit the script.", "error"); return;
                }
                 if (selectedService === 'deepgram_nova' && (!DEEPGRAM_API_KEY || DEEPGRAM_API_KEY === "Token YOUR_DEEPGRAM_API_KEY_HERE" || DEEPGRAM_API_KEY.length < 20)) { 
                     updateStatus("Deepgram API Key not configured. Please edit the script.", "error"); return;
                }

                 if (recordingState !== 'idle') { 
                    updateStatus("Please stop any current recording process first.", "info"); return; 
                 } 
                 audioFileUpload.click(); 
            });
            audioFileUpload.addEventListener('change', async (event) => { 
                const file = event.target.files[0];
                if (file) {
                    fileNameDisplay.textContent = `Selected: ${file.name}`;
                    clearOutputFields();
                    updateStatus(`Preparing "${file.name}"...`, "info");
                    showLoading("uploading_prepare"); 
                    uploadAudioBtn.textContent = "Processing..."; 
                    uploadAudioBtn.classList.add('animating'); 
                    
                    const originalFileName = file.name;
                    const firebaseUploadedFilePath = `uploaded_audio/${originalFileName}`;

                    await saveAudioToDB(file, originalFileName); 
                    try {
                        showLoading("uploading_to_firebase"); 
                        await uploadAudioToFirebase(file, firebaseUploadedFilePath);
                    } catch (fbError) {
                         console.warn("Firebase upload failed for uploaded file, continuing locally:", fbError);
                         updateStatus("Cloud save failed for upload, proceeding locally.", "info");
                    } finally {
                        hideLoading();
                    }
                    
                    await processAndTranscribeAudio(file, originalFileName);
                } else { 
                    fileNameDisplay.textContent = "No file selected."; 
                    uploadAudioBtn.textContent = "Upload Audio File";
                    uploadAudioBtn.classList.remove('animating');
                }
                event.target.value = null; 
            });

            function displayClientSideTopics(text) { 
                const detected = new Set();
                const lowerText = text.toLowerCase();

                if (topicMedicalCheckbox.checked) {
                    for (const keyword of TOPIC_KEYWORDS_LOCAL.medical) {
                        if (lowerText.includes(keyword.toLowerCase())) {
                            detected.add("Medical (Singapore) - Keyword Match");
                            break; 
                        }
                    }
                }
                if (topicInsuranceCheckbox.checked) {
                     for (const keyword of TOPIC_KEYWORDS_LOCAL.insurance) {
                        if (lowerText.includes(keyword.toLowerCase())) {
                            detected.add("Insurance (Singapore) - Keyword Match");
                            break;
                        }
                    }
                }
                
                if (detected.size === 0 && topicGeneralCheckbox.checked) {
                    detected.add("General - Keyword Match");
                }

                detectedTopicsList.innerHTML = ''; 
                if (detected.size > 0) {
                    detectedTopicsList.classList.remove('empty');
                    detected.forEach(topic => {
                        const listItem = document.createElement('li');
                        listItem.textContent = topic;
                        detectedTopicsList.appendChild(listItem);
                    });
                } else {
                    detectedTopicsList.classList.add('empty');
                    detectedTopicsList.innerHTML = '<li>No focused topics detected by client-side keywords.</li>';
                }
            }
            
            function displayDeepgramTopics(dgTopics) {
                detectedTopicsList.innerHTML = '';
                if (dgTopics && dgTopics.length > 0) {
                    detectedTopicsList.classList.remove('empty');
                    dgTopics.forEach(topicObj => {
                        const listItem = document.createElement('li');
                        if (typeof topicObj === 'string') {
                           listItem.textContent = `Deepgram: ${topicObj}`;
                        } else if (topicObj.topic) {
                           listItem.textContent = `Deepgram: ${topicObj.topic} (Confidence: ${topicObj.confidence ? topicObj.confidence.toFixed(2) : 'N/A'})`;
                        }
                        detectedTopicsList.appendChild(listItem);
                    });
                } else {
                    detectedTopicsList.classList.add('empty');
                    detectedTopicsList.innerHTML = '<li>No topics detected by Deepgram.</li>';
                }
            }


            async function processAndTranscribeAudio(inputAudioBlob, originalFileName) {
                if (!loadingOverlay.classList.contains('visible')) {
                    showLoading("transcribing_chunk"); 
                }

                const targetButtonForAnimation = recordingState === 'stopped_for_processing' ? stopRecordBtn : uploadAudioBtn;
                targetButtonForAnimation.textContent = "Transcribing...";
                targetButtonForAnimation.classList.add('animating');

                const selectedSttService = sttServiceSelect.value;

                if (selectedSttService === 'openai_whisper') {
                    if (!OPENAI_API_KEY || OPENAI_API_KEY.includes("YOUR_OPENAI_API_KEY_HERE")) {
                        resetToIdle("OpenAI API Key not configured.", "error"); return;
                    }
                    await transcribeWithOpenAI(inputAudioBlob, originalFileName);
                } else if (selectedSttService === 'deepgram_nova') {
                    if (!DEEPGRAM_API_KEY || DEEPGRAM_API_KEY.includes("YOUR_DEEPGRAM_API_KEY_HERE")) {
                         resetToIdle("Deepgram API Key not configured.", "error"); return;
                    }
                    await transcribeWithDeepgram(inputAudioBlob, originalFileName);
                } else {
                    resetToIdle("Invalid STT service selected.", "error");
                }
                
                targetButtonForAnimation.classList.remove('animating');
            }

            async function transcribeWithOpenAI(inputAudioBlob, originalFileName) {
                let decodedAudioBuffer;
                try {
                    showLoading("decoding");
                    updateStatus(`Decoding "${originalFileName}" for OpenAI...`, "info");
                    const arrayBuffer = await inputAudioBlob.arrayBuffer();
                    if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    if (audioContext.state === 'suspended') await audioContext.resume();
                    decodedAudioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                } catch (e) {
                    console.error("Error decoding audio:", e);
                    resetToIdle(`Error decoding audio: ${e.message.substring(0,100)}`, "error"); return;
                }

                const totalDuration = decodedAudioBuffer.duration;
                if (totalDuration === Infinity || isNaN(totalDuration) || totalDuration <= 0) {
                    resetToIdle("Audio duration is invalid or zero after decoding.", "error"); return;
                }

                const numChunks = Math.ceil(totalDuration / MAX_CHUNK_DURATION_SECONDS_OPENAI);
                console.log(`Splitting "${originalFileName}" into ${numChunks} PCM chunks for OpenAI (Total Duration: ${totalDuration.toFixed(2)}s).`);
                
                let allTranscribedText = ""; 
                let allSegments = [];

                for (let i = 0; i < numChunks; i++) {
                    showLoading("transcribing_chunk"); 
                    updateStatus(`OpenAI: Transcribing chunk ${i + 1} of ${numChunks}...`, "info");

                    const chunkStartTimeAbsolute = i * MAX_CHUNK_DURATION_SECONDS_OPENAI;
                    const chunkEndTimeAbsolute = Math.min((i + 1) * MAX_CHUNK_DURATION_SECONDS_OPENAI, totalDuration);
                    const chunkDuration = chunkEndTimeAbsolute - chunkStartTimeAbsolute;
                    if (chunkDuration <= 0.1) { continue; }

                    const startSample = Math.floor(chunkStartTimeAbsolute * decodedAudioBuffer.sampleRate);
                    const endSample = Math.ceil(chunkEndTimeAbsolute * decodedAudioBuffer.sampleRate);
                    const chunkNumSamples = endSample - startSample;
                    if (chunkNumSamples <= 0) { continue; }
                    
                    const chunkAudioBuffer = audioContext.createBuffer(decodedAudioBuffer.numberOfChannels, chunkNumSamples, decodedAudioBuffer.sampleRate);
                    for (let channel = 0; channel < decodedAudioBuffer.numberOfChannels; channel++) {
                        chunkAudioBuffer.getChannelData(channel).set(decodedAudioBuffer.getChannelData(channel).slice(startSample, endSample));
                    }

                    const wavBlob = bufferToWave(chunkAudioBuffer, chunkNumSamples);
                    
                    const formData = new FormData();
                    formData.append("file", wavBlob, `chunk_${i+1}.wav`); 
                    formData.append("model", "whisper-1"); 
                    formData.append("response_format", "verbose_json"); 
                    formData.append("timestamp_granularities[]", "segment"); 

                    try {
                        const response = await fetch("https://api.openai.com/v1/audio/transcriptions", { 
                            method: "POST", headers: { "Authorization": `Bearer ${OPENAI_API_KEY}` }, body: formData 
                        });
                        const responseBodyText = await response.text(); 

                        if (!response.ok) { 
                            let errDetails = `OpenAI API Error (Chunk ${i+1}) - Status: ${response.status}.`; 
                            try { 
                                const errorJson = JSON.parse(responseBodyText);
                                if (errorJson.error && errorJson.error.message) errDetails = errorJson.error.message;
                            } catch (parseErr) {} 
                            throw new Error(errDetails); 
                        }
                        const data = JSON.parse(responseBodyText);
                        if (data.text) allTranscribedText += data.text + " "; 
                        if (data.segments && Array.isArray(data.segments)) {
                            data.segments.forEach(segment => {
                                allSegments.push({ 
                                    ...segment, 
                                    start: segment.start + chunkStartTimeAbsolute, 
                                    end: segment.end + chunkStartTimeAbsolute 
                                });
                            });
                        }
                    } catch (error) { 
                        console.error(`Error processing OpenAI chunk ${i + 1}:`, error); 
                        allTranscribedText += `[ERROR IN OPENAI CHUNK ${i+1}: ${error.message.substring(0,50)}] `; 
                    }
                } 

                showLoading("finalizing_transcription");
                updateStatus("Finalizing OpenAI transcription...", "info");

                transcriptOutputConvo.value = allTranscribedText.trim();
                let timestampedTextOutput = "";
                allSegments.sort((a,b) => a.start - b.start).forEach(segment => { 
                    timestampedTextOutput += `[${segment.start.toFixed(2)}s - ${segment.end.toFixed(2)}s] ${segment.text}\n`; 
                });
                transcriptOutputTimestamped.value = timestampedTextOutput.trim();
                transcriptOutputMultispeaker.value = timestampedTextOutput.trim() + "\n\n(Speaker labels not provided by OpenAI Whisper)";

                displayClientSideTopics(allTranscribedText); 
                
                if (allTranscribedText.includes("[ERROR IN OPENAI CHUNK")) { 
                    resetToIdle(`OpenAI transcription partially failed. Check console.`, "error"); 
                } else if (allTranscribedText.trim() === "" && allSegments.length === 0) { 
                    resetToIdle(`OpenAI: No speech detected or no text for "${originalFileName}".`, "info"); 
                } else { 
                    resetToIdle(`OpenAI transcription of "${originalFileName}" complete!`, "success"); 
                }
            }

            async function transcribeWithDeepgram(inputAudioBlob, originalFileName) {
                if (!DEEPGRAM_API_KEY || DEEPGRAM_API_KEY.includes("YOUR_DEEPGRAM_API_KEY_HERE")) {
                    resetToIdle("Deepgram API Key not configured. Please edit the script.", "error"); return;
                }

                showLoading("transcribing_chunk");
                updateStatus(`Deepgram: Processing "${originalFileName}"...`, "info");
                
                let audioToSend = inputAudioBlob;
                
                try {
                    const arrayBuffer = await inputAudioBlob.arrayBuffer();
                    const decodedFullAudio = await audioContext.decodeAudioData(arrayBuffer);
                    audioToSend = bufferToWave(decodedFullAudio, decodedFullAudio.length); 
                    console.log("Converted input to WAV for Deepgram, size:", audioToSend.size, "type:", audioToSend.type);
                } catch (decodeError) {
                    console.warn("Could not decode audio to PCM for WAV conversion, sending original blob to Deepgram:", decodeError);
                    audioToSend = inputAudioBlob;
                }


                const params = new URLSearchParams({
                    model: 'nova-3', 
                    diarize: 'true',
                    smart_format: 'true',
                    paragraphs: 'true',
                    sentiment: 'true',
                    detect_language: 'true',
                    topics: 'true', 
                    custom_topic_mode: 'strict' 
                });

                const customTopics = [];
                if (topicMedicalCheckbox.checked) customTopics.push('medical', 'doctor', 'health', 'medicine', 'patient', 'clinic', 'hospital', 'cancer', 'diabetes', 'flu', 'covid', 'vaccine', 'surgery'); 
                if (topicInsuranceCheckbox.checked) customTopics.push('insurance', 'policy', 'claim', 'premium', 'coverage', 'insurer', 'accident', 'disability', 'life', 'health');
                
                customTopics.forEach(topic => params.append('custom_topic', topic));

                const deepgramUrl = `https://api.deepgram.com/v1/listen?${params.toString()}`;

                try {
                    const response = await fetch(deepgramUrl, {
                        method: 'POST',
                        headers: {
                            'Authorization': DEEPGRAM_API_KEY,
                            'Content-Type': audioToSend.type || 'audio/wav' 
                        },
                        body: audioToSend
                    });

                    const responseBodyText = await response.text();

                    if (!response.ok) {
                        let errDetails = `Deepgram API Error - Status: ${response.status}.`;
                        try {
                            const errorJson = JSON.parse(responseBodyText);
                            if (errorJson.err_msg) errDetails = errorJson.err_msg;
                            else if (errorJson.reason) errDetails = errorJson.reason;
                            else if (errorJson.message) errDetails = errorJson.message;
                        } catch (e) { /* not JSON */ }
                        throw new Error(errDetails);
                    }

                    const data = JSON.parse(responseBodyText);
                    console.log("Deepgram Response:", data);

                    let fullTranscript = "";
                    let timestampedOutput = "";
                    let multispeakerOutput = "";

                    if (data.results && data.results.channels && data.results.channels.length > 0) {
                        const channel = data.results.channels[0];
                        if (channel.alternatives && channel.alternatives.length > 0) {
                            fullTranscript = channel.alternatives[0].transcript || "";
                            transcriptOutputConvo.value = fullTranscript;

                            if (channel.alternatives[0].paragraphs) {
                                const paragraphs = channel.alternatives[0].paragraphs.paragraphs;
                                paragraphs.forEach(para => {
                                    para.sentences.forEach(sentence => {
                                        timestampedOutput += `[${sentence.start.toFixed(2)}s - ${sentence.end.toFixed(2)}s] ${sentence.text}\n`;
                                    });
                                });
                                transcriptOutputTimestamped.value = timestampedOutput.trim();
                            } else if (channel.alternatives[0].words) { 
                                let currentSegment = "";
                                let segmentStartTime = -1;
                                channel.alternatives[0].words.forEach(word => {
                                    if (segmentStartTime === -1) segmentStartTime = word.start;
                                    currentSegment += word.punctuated_word ? word.punctuated_word + " " : word.word + " ";
                                    if (word.punctuated_word && word.punctuated_word.match(/[.?!]/)) { 
                                         timestampedOutput += `[${segmentStartTime.toFixed(2)}s - ${word.end.toFixed(2)}s] ${currentSegment.trim()}\n`;
                                         currentSegment = "";
                                         segmentStartTime = -1;
                                    }
                                });
                                if(currentSegment) timestampedOutput += `[${segmentStartTime.toFixed(2)}s - ${channel.alternatives[0].words.slice(-1)[0].end.toFixed(2)}s] ${currentSegment.trim()}\n`; 
                                transcriptOutputTimestamped.value = timestampedOutput.trim();
                            }

                            if (channel.alternatives[0].words && data.results.utterances) { 
                                data.results.utterances.forEach(utterance => {
                                    multispeakerOutput += `[Speaker ${utterance.speaker}: ${utterance.start.toFixed(2)}s - ${utterance.end.toFixed(2)}s] ${utterance.transcript}\n`;
                                });
                                transcriptOutputMultispeaker.value = multispeakerOutput.trim();
                            } else {
                                transcriptOutputMultispeaker.value = timestampedOutput.trim(); 
                            }
                        }
                        if (data.results.summary && data.results.summary.topics) {
                            displayDeepgramTopics(data.results.summary.topics);
                        } else if (data.results.topics) { 
                             displayDeepgramTopics(data.results.topics);
                        } else {
                            displayClientSideTopics(fullTranscript); 
                        }

                    } else {
                        transcriptOutputConvo.value = "No transcription result from Deepgram.";
                    }
                    
                    if (!fullTranscript.trim()) {
                        resetToIdle(`Deepgram: No speech detected or no text for "${originalFileName}".`, "info");
                    } else {
                         resetToIdle(`Deepgram transcription of "${originalFileName}" complete!`, "success");
                    }

                } catch (error) {
                    console.error('Error transcribing with Deepgram:', error);
                    resetToIdle(`Deepgram API Error: ${error.message.substring(0,150)}`, "error");
                }
            }
            
            function bufferToWave(audioBuffer, len) { 
                let numOfChan = audioBuffer.numberOfChannels, 
                    length = len * numOfChan * 2 + 44, 
                    buffer = new ArrayBuffer(length), 
                    view = new DataView(buffer), 
                    channels = [], i, sample, offset = 0, pos = 0;

                function writeString(view, offset, string) { 
                    for (let i = 0; i < string.length; i++) { view.setUint8(offset + i, string.charCodeAt(i)); } 
                }
                
                writeString(view, pos, 'RIFF'); pos += 4;
                view.setUint32(pos, length - 8, true); pos += 4; 
                writeString(view, pos, 'WAVE'); pos += 4;
                writeString(view, pos, 'fmt '); pos += 4;
                view.setUint32(pos, 16, true); pos += 4; 
                view.setUint16(pos, 1, true); pos += 2; 
                view.setUint16(pos, numOfChan, true); pos += 2;
                view.setUint32(pos, audioBuffer.sampleRate, true); pos += 4;
                view.setUint32(pos, audioBuffer.sampleRate * 2 * numOfChan, true); pos += 4; 
                view.setUint16(pos, numOfChan * 2, true); pos += 2; 
                view.setUint16(pos, 16, true); pos += 2; 
                writeString(view, pos, 'data'); pos += 4;
                view.setUint32(pos, length - pos - 4, true); pos += 4; 

                for (i = 0; i < audioBuffer.numberOfChannels; i++) {
                    channels.push(audioBuffer.getChannelData(i));
                }

                while (pos < length) {
                    for (i = 0; i < numOfChan; i++) {             
                        sample = Math.max(-1, Math.min(1, channels[i][offset])); 
                        sample = (sample < 0 ? sample * 0x8000 : sample * 0x7FFF); 
                        view.setInt16(pos, sample, true);          
                        pos += 2;
                    }
                    offset++;                                     
                }
                return new Blob([view], { type: 'audio/wav' });
            }

            resetToIdle(); 
        });
    </script>
</body>
</html>
```

---

**2. New `voice-agent.html` Page**

This new page will handle the live audio streaming STT, LLM chat completion, and TTS.

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Finla.ai - Voice Agent</title>
    <style>
        /* CSS Variables for Theming and Consistency */
        :root {
            --font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            
            /* Finla.ai Inspired Colors */
            --finla-dark-blue: #1E3A8A; 
            --finla-light-blue-accent: #60A5FA;
            --finla-green-accent: #34D399;

            /* Light Theme (Default) */
            --bg-primary-light: #F7F9FC;
            --bg-secondary-light: #FFFFFF;
            --text-primary-light: #222F3E;
            --text-secondary-light: #576574;
            --accent-primary-light: var(--finla-dark-blue);
            --accent-secondary-light: #1C3274;
            --border-color-light: #DDE3EA;
            --shadow-color-light: rgba(30, 58, 138, 0.08);
            --error-color-light: #EF4444;
            --success-color-light: #10B981;
            --output-bg-light: #FDFEFE;


            /* Dark Theme */
            --bg-primary-dark: #161A1D;
            --bg-secondary-dark: #1F2428;
            --text-primary-dark: #E5E7EB;
            --text-secondary-dark: #9CA3AF;
            --accent-primary-dark: var(--finla-light-blue-accent);
            --accent-secondary-dark: #3B82F6;
            --border-color-dark: #374151;
            --shadow-color-dark: rgba(0, 0, 0, 0.2);
            --error-color-dark: #F87171;
            --success-color-dark: #34D399;
            --output-bg-dark: #24292E;


            /* Universal Variables */
            --border-radius: 8px;
            --transition-speed: 0.25s;
            --button-padding: 0.75em 1.4em;
        }

        /* Initialize theme variables */
        body {
            --bg-primary: var(--bg-primary-light);
            --bg-secondary: var(--bg-secondary-light);
            --text-primary: var(--text-primary-light);
            --text-secondary: var(--text-secondary-light);
            --accent-primary: var(--accent-primary-light);
            --accent-secondary: var(--accent-secondary-light);
            --border-color: var(--border-color-light);
            --shadow-color: var(--shadow-color-light);
            --error-color: var(--error-color-light);
            --success-color: var(--success-color-light);
            --output-bg: var(--output-bg-light);
        }

        body.dark-theme {
            --bg-primary: var(--bg-primary-dark);
            --bg-secondary: var(--bg-secondary-dark);
            --text-primary: var(--text-primary-dark);
            --text-secondary: var(--text-secondary-dark);
            --accent-primary: var(--accent-primary-dark);
            --accent-secondary: var(--accent-secondary-dark);
            --border-color: var(--border-color-dark);
            --shadow-color: var(--shadow-color-dark);
            --error-color: var(--error-color-dark);
            --success-color: var(--success-color-dark);
            --output-bg: var(--output-bg-dark);
        }

        /* Global Styles */
        *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
        html { scroll-behavior: smooth; }
        body {
            font-family: var(--font-family); background-color: var(--bg-primary); color: var(--text-primary);
            transition: background-color var(--transition-speed) ease, color var(--transition-speed) ease;
            font-size: 16px; line-height: 1.6;
        }
        .container { width: 100%; max-width: 1200px; margin: 0 auto; padding: 1rem; }

        /* Header */
        .app-header {
            display: flex; justify-content: space-between; align-items: center; padding: 0.75rem 1rem;
            background-color: var(--bg-secondary); box-shadow: 0 2px 8px var(--shadow-color);
            border-bottom: 1px solid var(--border-color); position: sticky; top: 0; z-index: 1000;
            transition: background-color var(--transition-speed) ease, border-color var(--transition-speed) ease;
        }
        .header-actions {
            display: flex;
            align-items: center;
            gap: 1rem;
        }
        .voice-agent-link {
            display: flex;
            align-items: center;
            justify-content: center;
            width: 36px;
            height: 36px;
            border-radius: 50%;
            background-color: var(--accent-primary);
            color: white;
            text-decoration: none;
            transition: background-color 0.25s ease, transform 0.1s ease;
            box-shadow: 0 2px 4px var(--shadow-color-light);
        }
        .voice-agent-link:hover {
            background-color: var(--accent-secondary);
            transform: scale(1.05);
            box-shadow: 0 4px 8px var(--shadow-color-light);
        }
        body.dark-theme .voice-agent-link {
            box-shadow: 0 2px 4px var(--shadow-color-dark);
        }
        body.dark-theme .voice-agent-link:hover {
            box-shadow: 0 4px 8px var(--shadow-color-dark);
        }
        .voice-agent-link svg {
            width: 20px;
            height: 20px;
            fill: currentColor;
        }

        .theme-switch-wrapper { display: flex; align-items: center; }
        .theme-switch { display: inline-block; height: 26px; position: relative; width: 50px; margin-left: 0.5rem; }
        .theme-switch input { display:none; }
        .slider {
            background-color: #B0B0B0; bottom: 0; cursor: pointer; left: 0; position: absolute; right: 0; top: 0;
            transition: .4s; border-radius: 26px;
        }
        .slider:before {
            background-color: #fff; bottom: 3px; content: ""; height: 20px; left: 3px; position: absolute;
            transition: .4s; width: 20px; border-radius: 50%; box-shadow: 0 1px 3px rgba(0,0,0,0.2);
        }
        input:checked + .slider { background-color: var(--accent-primary); }
        input:checked + .slider:before { transform: translateX(24px); }
        .main-content { padding-top: 1.5rem; display: flex; flex-direction: column; gap: 1.5rem; }
        .card {
            background-color: var(--bg-secondary); border-radius: var(--border-radius); padding: 1.5rem;
            box-shadow: 0 4px 12px var(--shadow-color); border: 1px solid var(--border-color);
            transition: background-color var(--transition-speed) ease, border-color var(--transition-speed) ease, box-shadow var(--transition-speed) ease;
        }
        .card-title { font-size: 1.1rem; font-weight: 600; margin-bottom: 1rem; color: var(--text-primary); }
        .controls-area .control-group { margin-bottom: 1rem; }
        .controls-area label { display: block; font-size: 0.875rem; color: var(--text-secondary); margin-bottom: 0.4rem; }
        .controls-area select, .status-display-text { 
            width: 100%; padding: 0.65rem 0.75rem; border-radius: var(--border-radius); border: 1px solid var(--border-color);
            background-color: var(--bg-primary); color: var(--text-primary); font-size: 0.9rem;
            transition: border-color var(--transition-speed) ease, background-color var(--transition-speed) ease, color var(--transition-speed) ease;
        }
        .controls-area select:focus { outline: none; border-color: var(--accent-primary); box-shadow: 0 0 0 2px var(--accent-primary-light); }
        body.dark-theme .controls-area select:focus { box-shadow: 0 0 0 2px var(--accent-primary-dark); }
        
        .status-display-container { opacity: 1; transform: translateY(0); transition: opacity var(--transition-speed) ease, transform var(--transition-speed) ease; }
        .status-display-container.hidden { opacity: 0; transform: translateY(-10px); height: 0; overflow: hidden; }
        .status-display-text {
             min-height: 38px; display: flex; align-items: center; font-style: italic; margin-bottom: 0.5rem;
        }
        .status-display-text.error { color: var(--error-color); font-weight: 500; border-left: 3px solid var(--error-color); padding-left: 0.5rem;}
        .status-display-text.success { color: var(--success-color); font-weight: 500; border-left: 3px solid var(--success-color); padding-left: 0.5rem;}
        #audioWaveformCanvas {
            width: 100%; height: 60px; background-color: var(--output-bg);
            border-radius: calc(var(--border-radius) / 2); display: none; margin-top: 0.5rem; border: 1px solid var(--border-color);
        }
        .button-group { display: flex; flex-direction: column; gap: 0.75rem; }
        .button-row { display: flex; gap: 0.5rem; } 
        .button-row .btn { flex: 1; } 

        .btn {
            padding: var(--button-padding); font-size: 0.95rem; font-weight: 500; border: none;
            border-radius: var(--border-radius); cursor: pointer;
            transition: background-color var(--transition-speed) ease, transform 0.1s ease, box-shadow var(--transition-speed) ease;
            text-align: center; width: 100%;
            box-shadow: 0 2px 4px var(--shadow-color-light);
        }
        body.dark-theme .btn { box-shadow: 0 2px 4px var(--shadow-color-dark); }
        .btn-primary { background-color: var(--accent-primary); color: white; }
        .btn-primary:hover:not(:disabled) { background-color: var(--accent-secondary); box-shadow: 0 4px 8px var(--shadow-color-light); }
        body.dark-theme .btn-primary:hover:not(:disabled) { box-shadow: 0 4px 8px var(--shadow-color-dark); }
        .btn-secondary { background-color: var(--bg-secondary); color: var(--accent-primary); border: 1.5px solid var(--accent-primary); }
        .btn-secondary:hover:not(:disabled) { background-color: var(--accent-primary); color: white; box-shadow: 0 4px 8px var(--shadow-color-light); }
        body.dark-theme .btn-secondary:hover:not(:disabled) { background-color: var(--accent-primary); color: white; box-shadow: 0 4px 8px var(--shadow-color-dark); }
        .btn-warning { background-color: #F59E0B; color: white; } 
        .btn-warning:hover:not(:disabled) { background-color: #D97706; } 

        .btn:active:not(:disabled) { transform: scale(0.98); box-shadow: 0 1px 2px var(--shadow-color-light); }
        body.dark-theme .btn:active:not(:disabled) { box-shadow: 0 1px 2px var(--shadow-color-dark); }
        .btn.disabled, .btn:disabled {
            background-color: var(--text-secondary-light) !important; color: var(--bg-secondary-light) !important;
            cursor: not-allowed !important; opacity: 0.5 !important; border-color: var(--text-secondary-light) !important;
            box-shadow: none !important;
        }
        body.dark-theme .btn.disabled, body.dark-theme .btn:disabled {
             background-color: var(--text-secondary-dark) !important; color: var(--bg-secondary-dark) !important;
             border-color: var(--text-secondary-dark) !important;
        }
        .upload-group { margin-top: 0.75rem; }
        #audioFileUpload { display: none; }
        .btn-upload { background-color: var(--finla-green-accent); color: white; }
        .btn-upload:hover:not(:disabled) { background-color: #25a575; box-shadow: 0 4px 8px var(--shadow-color-light);}
        body.dark-theme .btn-upload:hover:not(:disabled) { box-shadow: 0 4px 8px var(--shadow-color-dark); }
        #fileNameDisplay { font-size: 0.8rem; color: var(--text-secondary); margin-top: 0.4rem; text-align: center; word-break: break-all; }
        .output-column { display: flex; flex-direction: column; gap: 1.5rem; }
        .output-area {
            width: 100%; min-height: 120px; padding: 0.75rem 1rem; border-radius: var(--border-radius);
            border: 1px solid var(--border-color); background-color: var(--output-bg); color: var(--text-primary);
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
            font-size: 0.875rem; line-height: 1.6; resize: vertical;
            transition: border-color var(--transition-speed) ease, background-color var(--transition-speed) ease;
            white-space: pre-wrap; word-wrap: break-word;
        }
        .output-area:focus { outline: none; border-color: var(--accent-primary); box-shadow: 0 0 0 2px var(--accent-primary-light); }
        body.dark-theme .output-area:focus { box-shadow: 0 0 0 2px var(--accent-primary-dark); }
        
        #conversationLog {
            width: 100%; min-height: 300px; padding: 1rem; border-radius: var(--border-radius);
            border: 1px solid var(--border-color); background-color: var(--output-bg); color: var(--text-primary);
            font-family: var(--font-family); font-size: 0.9rem; line-height: 1.6;
            overflow-y: auto; display: flex; flex-direction: column; gap: 0.75rem;
        }
        .message {
            padding: 0.6rem 1rem; border-radius: 12px; max-width: 80%;
            word-wrap: break-word; white-space: pre-wrap;
        }
        .user-message {
            background-color: var(--accent-primary);
            color: white;
            align-self: flex-end;
            border-bottom-right-radius: 4px;
        }
        .ai-message {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            align-self: flex-start;
            border: 1px solid var(--border-color);
            border-bottom-left-radius: 4px;
        }
        .ai-message.speaking {
            animation: pulse-ai-speaking 1s infinite alternate;
        }
        @keyframes pulse-ai-speaking {
            from { box-shadow: 0 0 0 0px var(--accent-light-blue-accent); }
            to { box-shadow: 0 0 0 5px rgba(96, 165, 250, 0.4); } /* Use a lighter version of accent for pulse */
        }
        body.dark-theme .ai-message.speaking {
            animation: pulse-ai-speaking-dark 1s infinite alternate;
        }
        @keyframes pulse-ai-speaking-dark {
            from { box-shadow: 0 0 0 0px var(--accent-primary); }
            to { box-shadow: 0 0 0 5px rgba(59, 130, 246, 0.4); } /* Use a lighter version of accent for pulse */
        }

        #startStopLiveBtn {
            font-size: 1.1rem;
            padding: 1rem 2rem;
            margin-top: 1rem;
        }
        #liveStatus {
            font-size: 0.9rem;
            color: var(--text-secondary);
            text-align: center;
            margin-top: 0.5rem;
            min-height: 20px;
        }
        .status-bar {
            height: 5px;
            background-color: var(--border-color);
            border-radius: 2.5px;
            overflow: hidden;
            margin-top: 0.5rem;
        }
        .status-bar-fill {
            height: 100%;
            background-color: var(--accent-primary);
            width: 0%;
            transition: width 0.1s linear;
        }

        #listeningIndicator {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-top: 1rem;
            height: 40px;
            background-color: var(--output-bg);
            border-radius: var(--border-radius);
            border: 1px solid var(--border-color);
            font-size: 0.9rem;
            color: var(--text-secondary);
        }
        #listeningIndicator.active {
            color: var(--accent-primary);
            font-weight: 600;
        }
        #listeningIndicator span {
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: var(--accent-primary);
            border-radius: 50%;
            margin: 0 3px;
            opacity: 0.3;
        }
        #listeningIndicator.active span:nth-child(1) { animation: grow-shrink 1.2s infinite ease-in-out 0s; }
        #listeningIndicator.active span:nth-child(2) { animation: grow-shrink 1.2s infinite ease-in-out 0.2s; }
        #listeningIndicator.active span:nth-child(3) { animation: grow-shrink 1.2s infinite ease-in-out 0.4s; }

        @keyframes grow-shrink {
            0%, 100% { transform: scale(0.6); opacity: 0.3; }
            50% { transform: scale(1.2); opacity: 1; }
        }

    </style>
</head>
<body>
    <header class="app-header">
        <a href="index.html" class="logo-link">
            <img src="https://finla.ai/img/logo_text.svg" alt="Finla.ai Logo" class="logo-img">
        </a>
        <div class="header-actions">
            <div class="theme-switch-wrapper">
                <label for="themeToggleCheckbox" style="color: var(--text-secondary); font-size: 0.8rem; margin-right: 0.5rem;">Dark Mode</label>
                <label class="theme-switch" for="themeToggleCheckbox">
                    <input type="checkbox" id="themeToggleCheckbox" />
                    <div class="slider"></div>
                </label>
            </div>
        </div>
    </header>

    <div class="container">
        <main class="main-content">
            <div class="controls-column">
                <section class="card controls-area">
                    <h2 class="card-title">Voice Agent Controls</h2>
                    <div class="control-group">
                        <label for="llmModelSelect">LLM Model (OpenRouter)</label>
                        <select id="llmModelSelect">
                            <option value="meta-llama/llama-4-maverick:free">Llama-4-Maverick (Free)</option>
                            <option value="openai/gpt-4o">GPT-4o (OpenRouter)</option>
                            <!-- Add more OpenRouter models as needed -->
                        </select>
                    </div>
                    <div class="control-group">
                        <label for="ttsVoiceSelect">TTS Voice (OpenAI Realtime)</label>
                        <select id="ttsVoiceSelect">
                            <option value="alloy">Alloy</option>
                            <option value="fable">Fable</option>
                            <option value="onyx">Onyx</option>
                            <option value="nova">Nova</option>
                            <option value="shimmer">Shimmer</option>
                            <option value="echo">Echo</option>
                        </select>
                    </div>
                    <button id="startStopLiveBtn" class="btn btn-primary">Start Voice Agent</button>
                    <div id="liveStatus">Idle.</div>
                    <div id="listeningIndicator">
                        <span></span><span></span><span></span>
                    </div>
                </section>
            </div>

            <div class="transcript-column-wrapper output-column">
                <section class="card">
                    <h2 class="card-title">Conversation Log</h2>
                    <div id="conversationLog">
                        <div class="ai-message message">Hello! I am your AI voice assistant. How can I help you today?</div>
                    </div>
                </section>
            </div>
        </main>
    </div>

    <footer class="app-footer">
        <p>Â© <span id="currentYear"></span> Finla.ai. Voice Agent Demo.</p>
    </footer>

    <script>
        // AudioWorklet Processor for microphone input
        // This is a simple pass-through to get PCM data.
        const audioWorkletCode = `
            class MicProcessor extends AudioWorkletProcessor {
                process(inputs, outputs, parameters) {
                    const input = inputs[0];
                    if (input.length > 0) {
                        const monoChannel = input[0]; // Assuming mono input from microphone
                        this.port.postMessage(monoChannel.buffer, [monoChannel.buffer]);
                    }
                    return true;
                }
            }
            registerProcessor('mic-processor', MicProcessor);
        `;
        const audioWorkletBlob = new Blob([audioWorkletCode], { type: 'application/javascript' });
        const audioWorkletUrl = URL.createObjectURL(audioWorkletBlob);


        document.addEventListener('DOMContentLoaded', async () => {
            // --- API Keys ---
            // These keys are exposed in client-side code. This is ONLY acceptable for local development and testing.
            // For any production or publicly shared environment, these keys MUST be handled by a backend proxy.
            const OPENAI_API_KEY = "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"; 
            const OPENROUTER_API_KEY = "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"; 
            // --- --- --- ---

            const themeToggleCheckbox = document.getElementById('themeToggleCheckbox');
            const startStopLiveBtn = document.getElementById('startStopLiveBtn');
            const liveStatusElement = document.getElementById('liveStatus');
            const conversationLog = document.getElementById('conversationLog');
            const llmModelSelect = document.getElementById('llmModelSelect');
            const ttsVoiceSelect = document.getElementById('ttsVoiceSelect');
            const listeningIndicator = document.getElementById('listeningIndicator');

            let audioContext;
            let mediaStream;
            let micNode;
            let audioInputQueue = []; // Queue for audio data to send over WebSocket
            let websocket;
            let isConnected = false;
            let audioOutputSource; // For playing TTS audio

            // Conversation history for LLM
            let conversationHistory = [{
                role: "assistant",
                content: "Hello! I am your AI voice assistant. How can I help you today?"
            }];

            // --- Theme Management (copied from index.html for consistency) ---
            function applyTheme(theme) {
                if (theme === 'dark') {
                    document.body.classList.add('dark-theme');
                    themeToggleCheckbox.checked = true;
                } else {
                    document.body.classList.remove('dark-theme');
                    themeToggleCheckbox.checked = false;
                }
            }
            const savedTheme = localStorage.getItem('theme');
            if (savedTheme) applyTheme(savedTheme);
            else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) applyTheme('dark');
            else applyTheme('light'); 
            themeToggleCheckbox.addEventListener('change', function() {
                const theme = this.checked ? 'dark' : 'light';
                applyTheme(theme);
                localStorage.setItem('theme', theme);
            });
            // Set current year in footer
            document.getElementById('currentYear').textContent = new Date().getFullYear();

            // --- UI Update Functions ---
            function updateLiveStatus(message, isError = false) {
                liveStatusElement.textContent = message;
                liveStatusElement.style.color = isError ? 'var(--error-color)' : 'var(--text-secondary)';
                console.log(`Live Status: ${message}`);
            }

            function addMessageToLog(role, content) {
                const messageDiv = document.createElement('div');
                messageDiv.classList.add('message');
                messageDiv.classList.add(role === 'user' ? 'user-message' : 'ai-message');
                messageDiv.textContent = content;
                conversationLog.appendChild(messageDiv);
                conversationLog.scrollTop = conversationLog.scrollHeight; // Scroll to bottom
            }

            function setListeningIndicator(active) {
                if (active) {
                    listeningIndicator.classList.add('active');
                    listeningIndicator.textContent = 'Listening... ';
                    // Add spans dynamically if not present
                    if (listeningIndicator.children.length === 0) {
                        for(let i=0; i<3; i++) {
                            listeningIndicator.appendChild(document.createElement('span'));
                        }
                    }
                } else {
                    listeningIndicator.classList.remove('active');
                    listeningIndicator.textContent = 'Not Listening';
                    // Remove spans
                    while (listeningIndicator.firstChild) {
                        listeningIndicator.removeChild(listeningIndicator.lastChild);
                    }
                }
            }


            // --- WebSocket & Audio Streaming Logic ---
            async function initRealtimeSession() {
                if (isConnected) {
                    console.warn("Already connected to realtime session.");
                    return;
                }

                if (!OPENAI_API_KEY || OPENAI_API_KEY.includes("YOUR_OPENAI_API_KEY_HERE")) {
                    updateLiveStatus("OpenAI API Key not configured.", true);
                    return;
                }
                if (!OPENROUTER_API_KEY || OPENROUTER_API_KEY.includes("YOUR_OPENROUTER_API_KEY_HERE")) {
                    updateLiveStatus("OpenRouter API Key not configured.", true);
                    return;
                }

                updateLiveStatus("Connecting to OpenAI Realtime API...");
                startStopLiveBtn.disabled = true;
                setListeningIndicator(false);

                try {
                    // 1. Create Realtime Session
                    const sessionResponse = await fetch("https://api.openai.com/v1/realtime/sessions", {
                        method: "POST",
                        headers: {
                            "Authorization": `Bearer ${OPENAI_API_KEY}`,
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify({
                            model: "gpt-4o-realtime-preview",
                            modalities: ["audio", "text"],
                            instructions: "You are a friendly assistant.",
                            voice: ttsVoiceSelect.value, // Use selected TTS voice
                            input_audio_format: "pcm16", // Specify 16-bit PCM for microphone
                            output_audio_format: "pcm16",
                            input_audio_transcription: { model: "whisper-1" },
                        }),
                    });

                    if (!sessionResponse.ok) {
                        const errorBody = await sessionResponse.json();
                        throw new Error(`Failed to create OpenAI Realtime session: ${sessionResponse.status} - ${errorBody.error?.message || JSON.stringify(errorBody)}`);
                    }

                    const sessionData = await sessionResponse.json();
                    const websocketUrl = sessionResponse.headers.get("x-openai-sess-url");

                    if (!websocketUrl) {
                        throw new Error("WebSocket URL not found in session response headers.");
                    }

                    // 2. Get Microphone Stream
                    mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    await audioContext.audioWorklet.addModule(audioWorkletUrl); // Add custom AudioWorklet processor

                    const source = audioContext.createMediaStreamSource(mediaStream);
                    micNode = new AudioWorkletNode(audioContext, 'mic-processor', {
                        processorOptions: { sampleRate: audioContext.sampleRate }
                    });
                    
                    source.connect(micNode);
                    micNode.connect(audioContext.destination); // Connect to destination to keep it alive (required by spec)

                    micNode.port.onmessage = (event) => {
                        if (websocket && websocket.readyState === WebSocket.OPEN) {
                            const pcmData = new Int16Array(event.data);
                            // Convert Float32Array (from AudioWorklet) to Int16Array (PCM16)
                            const int16Data = new Int16Array(pcmData.length);
                            for (let i = 0; i < pcmData.length; i++) {
                                int16Data[i] = Math.max(-1, Math.min(1, pcmData[i])) * 32767; // Normalize to Int16 range
                            }
                            websocket.send(int16Data.buffer);
                        } else {
                            audioInputQueue.push(event.data); // Queue if WS not ready
                        }
                    };

                    // 3. Establish WebSocket connection
                    websocket = new WebSocket(websocketUrl);

                    websocket.onopen = () => {
                        isConnected = true;
                        updateLiveStatus("Connected. Listening...");
                        startStopLiveBtn.textContent = "Stop Voice Agent";
                        startStopLiveBtn.classList.remove('animating'); // Ensure no lingering animation
                        startStopLiveBtn.disabled = false;
                        setListeningIndicator(true);

                        // Send any queued audio data
                        while(audioInputQueue.length > 0) {
                            const pcmData = new Int16Array(audioInputQueue.shift());
                            const int16Data = new Int16Array(pcmData.length);
                            for (let i = 0; i < pcmData.length; i++) {
                                int16Data[i] = Math.max(-1, Math.min(1, pcmData[i])) * 32767;
                            }
                            websocket.send(int16Data.buffer);
                        }
                    };

                    let currentAITextBuffer = ""; // To accumulate AI response text
                    let aiMessageElement = null; // Reference to the current AI message div

                    websocket.onmessage = async (event) => {
                        const message = JSON.parse(event.data);

                        if (message.type === 'speech') {
                            // User's speech transcribed
                            console.log("User Speech:", message.text);
                            if (message.text && message.text.trim().length > 0) {
                                addMessageToLog('user', message.text);
                                conversationHistory.push({ role: "user", content: message.text });
                                // Immediately send to LLM
                                await callOpenRouterLLM(message.text);
                            }
                            setListeningIndicator(true); // Always listening unless AI is speaking or session ended
                        } else if (message.type === 'audio_response') {
                            // AI's audio response (TTS)
                            if (audioContext.state === 'suspended') await audioContext.resume();

                            const audioBuffer = await audioContext.decodeAudioData(message.audio_data);
                            audioOutputSource = audioContext.createBufferSource();
                            audioOutputSource.buffer = audioBuffer;
                            audioOutputSource.connect(audioContext.destination);
                            audioOutputSource.start(0);

                            setListeningIndicator(false); // Stop listening while AI speaks
                            if (aiMessageElement) {
                                aiMessageElement.classList.add('speaking'); // Add pulsing animation
                            }

                            audioOutputSource.onended = () => {
                                console.log("AI finished speaking.");
                                setListeningIndicator(true); // Resume listening after AI speaks
                                if (aiMessageElement) {
                                    aiMessageElement.classList.remove('speaking');
                                    aiMessageElement = null; // Clear reference
                                }
                            };
                        } else if (message.type === 'speak_response') {
                            // AI's text response for TTS
                            if (message.text) {
                                currentAITextBuffer += message.text;
                                if (!aiMessageElement || aiMessageElement.dataset.final !== 'false') { // Create new or update existing
                                    aiMessageElement = document.createElement('div');
                                    aiMessageElement.classList.add('message', 'ai-message');
                                    aiMessageElement.dataset.final = 'false'; // Mark as not final yet
                                    conversationLog.appendChild(aiMessageElement);
                                }
                                aiMessageElement.textContent = currentAITextBuffer;
                                conversationLog.scrollTop = conversationLog.scrollHeight;
                            }
                            if (message.is_final) {
                                console.log("AI Final Text:", currentAITextBuffer);
                                conversationHistory.push({ role: "assistant", content: currentAITextBuffer });
                                if (aiMessageElement) {
                                    aiMessageElement.dataset.final = 'true';
                                }
                                currentAITextBuffer = ""; // Reset buffer
                            }
                        } else {
                            console.log("Received unknown message type:", message.type, message);
                        }
                    };

                    websocket.onclose = (event) => {
                        isConnected = false;
                        updateLiveStatus(`Disconnected: ${event.code} ${event.reason}.`, true);
                        startStopLiveBtn.textContent = "Start Voice Agent";
                        startStopLiveBtn.disabled = false;
                        startStopLiveBtn.classList.remove('animating');
                        setListeningIndicator(false);
                        cleanupAudio();
                        console.error("WebSocket closed:", event);
                    };

                    websocket.onerror = (event) => {
                        isConnected = false;
                        updateLiveStatus("WebSocket error. See console for details.", true);
                        startStopLiveBtn.textContent = "Start Voice Agent";
                        startStopLiveBtn.disabled = false;
                        startStopLiveBtn.classList.remove('animating');
                        setListeningIndicator(false);
                        cleanupAudio();
                        console.error("WebSocket error:", event);
                    };

                } catch (error) {
                    console.error("Error initializing realtime session:", error);
                    updateLiveStatus(`Failed to start: ${error.message.substring(0,100)}`, true);
                    startStopLiveBtn.textContent = "Start Voice Agent";
                    startStopLiveBtn.disabled = false;
                    startStopLiveBtn.classList.remove('animating');
                    setListeningIndicator(false);
                    cleanupAudio();
                }
            }

            function cleanupAudio() {
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                    mediaStream = null;
                }
                if (micNode) {
                    micNode.disconnect();
                    micNode.port.onmessage = null;
                    micNode = null;
                }
                if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close();
                    audioContext = null;
                }
            }

            function terminateRealtimeSession() {
                if (websocket && websocket.readyState === WebSocket.OPEN) {
                    updateLiveStatus("Disconnecting...");
                    websocket.close(); 
                } else if (websocket && websocket.readyState === WebSocket.CONNECTING) {
                    websocket.close(); // Force close if still connecting
                }
                isConnected = false;
                startStopLiveBtn.textContent = "Stopping...";
                startStopLiveBtn.disabled = true;
                startStopLiveBtn.classList.add('animating');
                setListeningIndicator(false);
                cleanupAudio();
                updateLiveStatus("Disconnected.", false);
                startStopLiveBtn.textContent = "Start Voice Agent";
                startStopLiveBtn.disabled = false;
                startStopLiveBtn.classList.remove('animating');
            }

            // --- LLM Integration (OpenRouter) ---
            async function callOpenRouterLLM(userText) {
                if (!userText || userText.trim() === "") return;

                updateLiveStatus("Thinking...", false);
                setListeningIndicator(false);

                // OpenRouter supports 'messages' array like OpenAI
                const messagesToSend = conversationHistory.map(msg => ({
                    role: msg.role,
                    content: msg.content
                }));

                try {
                    const response = await fetch("https://openrouter.ai/api/v1/chat/completions", {
                        method: "POST",
                        headers: {
                            "Authorization": `Bearer ${OPENROUTER_API_KEY}`,
                            "Content-Type": "application/json",
                            // "HTTP-Referer": "YOUR_URL", // Optional, recommended for production
                            // "X-Title": "YOUR_APP_NAME", // Optional, recommended for production
                        },
                        body: JSON.stringify({
                            model: llmModelSelect.value, // Use selected LLM model
                            messages: messagesToSend,
                        }),
                    });

                    if (!response.ok) {
                        const errorBody = await response.json();
                        throw new Error(`OpenRouter API error: ${response.status} - ${errorBody.error?.message || JSON.stringify(errorBody)}`);
                    }

                    const data = await response.json();
                    const aiResponseText = data.choices[0].message.content;
                    console.log("LLM Response:", aiResponseText);

                    // Send AI response to OpenAI Realtime for TTS
                    if (websocket && websocket.readyState === WebSocket.OPEN) {
                        websocket.send(JSON.stringify({
                            type: 'speak',
                            text: aiResponseText,
                        }));
                    } else {
                        console.warn("WebSocket not open, cannot send speak request for AI response.");
                        // Fallback: display text directly if TTS can't play
                        addMessageToLog('assistant', aiResponseText);
                        conversationHistory.push({ role: "assistant", content: aiResponseText });
                        updateLiveStatus("LLM responded, but cannot speak.", true);
                        setListeningIndicator(true); // Resume listening
                    }

                } catch (error) {
                    console.error("Error calling OpenRouter LLM:", error);
                    updateLiveStatus(`LLM error: ${error.message.substring(0,100)}`, true);
                    // Add error message to log
                    addMessageToLog('assistant', `Error processing your request: ${error.message.substring(0, 150)}`);
                    conversationHistory.push({ role: "assistant", content: `Error processing your request.` });
                    setListeningIndicator(true); // Resume listening
                }
            }


            // --- Event Listeners ---
            startStopLiveBtn.addEventListener('click', () => {
                if (isConnected) {
                    terminateRealtimeSession();
                } else {
                    initRealtimeSession();
                }
            });

            // Initial UI state
            updateLiveStatus("Idle.");
            setListeningIndicator(false);
        });
    </script>
</body>
</html>
